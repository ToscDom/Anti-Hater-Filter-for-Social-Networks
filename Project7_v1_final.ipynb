{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a53faab3",
      "metadata": {
        "id": "a53faab3"
      },
      "source": [
        "# Anti-Hater Filter for Social Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06947562",
      "metadata": {
        "id": "06947562"
      },
      "source": [
        "Develop an automated moderation system based on Deep Learning techniques capable of identifying and filtering toxic, offensive, or inappropriate comments in real-time within an online platform.\n",
        "The model should classify comments into multiple relevant categories, ensuring effective management of harmful content without compromising the user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a410e280",
      "metadata": {
        "id": "a410e280"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CGHmik_wDQT4",
      "metadata": {
        "id": "CGHmik_wDQT4"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6C2Gqoc5DRDM",
      "metadata": {
        "id": "6C2Gqoc5DRDM"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fl_5CYodDTxR",
      "metadata": {
        "id": "fl_5CYodDTxR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfc5830",
      "metadata": {
        "id": "1cfc5830"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Conv1D, Dropout, MaxPooling1D, GRU, Bidirectional, concatenate, Dense, BatchNormalization\n",
        "from keras.backend import clear_session\n",
        "from keras.models import load_model\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88077c36",
      "metadata": {
        "id": "88077c36"
      },
      "source": [
        "## Import dataset and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65992e1",
      "metadata": {
        "id": "e65992e1"
      },
      "source": [
        "Textual comments must be converted into numerical sequences (tokenization).\n",
        "\n",
        "The data must be normalized and balanced to ensure that all categories of toxicity are represented equally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f74a585",
      "metadata": {
        "id": "5f74a585"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/\"\n",
        "df_bck = pd.read_csv(BASE_URL+\"Filter_Toxic_Comments_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f428b4f",
      "metadata": {
        "id": "9f428b4f"
      },
      "outputs": [],
      "source": [
        "df_bck.head()\n",
        "df_bck.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1cd1b03",
      "metadata": {
        "id": "a1cd1b03"
      },
      "source": [
        "The dataset contains 159,571 comments, with each comment labeled in one or more categories. Comments may have zero or more active labels.\n",
        "\n",
        "The six categories are:\n",
        "\n",
        "1) Toxic\n",
        "2) Severely Toxic\n",
        "3) Obscene\n",
        "4) Threat\n",
        "5) Insult\n",
        "6) Identity Hate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2802e280",
      "metadata": {
        "id": "2802e280"
      },
      "outputs": [],
      "source": [
        "df =df_bck.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb14519",
      "metadata": {
        "id": "ecb14519"
      },
      "source": [
        "Preprocessing of the text  by removing URLs, HTML tags, non-alphabetic characters, tokenizing, removing stopwords, and lemmatizing the words. The second stage of the text preprocessing involves instead the removal of the stopwords and the lemmatization of each token in both train and test sets:\n",
        "- Stop words are words that are commonly used but do not carry much meaning on their own, such as \"the\" and \"and\". Removing these words can help to simplify the text and improve model performance.\n",
        "- lemmatization, insetad, is the process of reducing words to their base form (or lemma). For example, the word \"running\" could be reduced to its base form \"run\". Lemmatization can help to normalize the text and reduce the number of unique words that a model needs to learn.\n",
        "\n",
        "To accomplish these two operations I exploited the `NLTK` library:\n",
        "\n",
        "- The `stopwords` module from NLTK contains a list of common English stop words that can be removed from text during preprocessing.\n",
        "- The `WordNetLemmatizer` class from NLTK is used to perform lemmatization\n",
        "\n",
        "The preprocess_text function tokenizes the input text using word_tokenize from NLTK. It then removes stop words using the stopwords module and lemmatizes the remaining tokens using WordNetLemmatizer. Finally, it joins the lemmatized tokens back into a string and returns the preprocessed text.\n",
        "\n",
        "Furthermore removing rows from the DataFrame where the column contains only spaces or is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46928ca0",
      "metadata": {
        "id": "46928ca0"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the given text by removing URLs, HTML tags, non-alphabetic characters,\n",
        "    tokenizing, removing stopwords, and lemmatizing the words.\n",
        "\n",
        "    :param text: A string containing the text to be preprocessed.\n",
        "    :return: A string of the preprocessed text.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Reconstruct the preprocessed text\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_empty_or_space_only_rows(data):\n",
        "    \"\"\"\n",
        "    Removes rows from the DataFrame where the 'cleaned_comment_text' column contains only spaces or is empty.\n",
        "\n",
        "    :param data: A pandas DataFrame expected to contain a column named 'cleaned_comment_text'.\n",
        "    :return: A tuple containing two DataFrames:\n",
        "             1. The rows that were removed (having only spaces or being empty in 'cleaned_comment_text').\n",
        "             2. The cleaned DataFrame with these rows removed.\n",
        "    \"\"\"\n",
        "    # Identify rows with 'cleaned_comment_text' that are either empty or contain only spaces\n",
        "    rows_with_only_spaces = data[data['cleaned_comment_text'].apply(lambda x: x.isspace() or not x)]\n",
        "\n",
        "    # Remove these rows from the dataset\n",
        "    data = data[~data['cleaned_comment_text'].apply(lambda x: x.isspace() or not x)]\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68aa924",
      "metadata": {
        "id": "c68aa924"
      },
      "outputs": [],
      "source": [
        "df['cleaned_comment_text'] = df['comment_text'].apply(preprocess_text)\n",
        "df[['comment_text', 'cleaned_comment_text']].head()\n",
        "remove_empty_or_space_only_rows(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8552340",
      "metadata": {
        "id": "d8552340"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['sum_injurious'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152976d7",
      "metadata": {
        "id": "152976d7"
      },
      "source": [
        "Verifing the categories contain only 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2a0458",
      "metadata": {
        "id": "cf2a0458"
      },
      "outputs": [],
      "source": [
        "for col in df.columns[1:-1]:\n",
        "    print(f\"Column '{col}': {df[col].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a7e915",
      "metadata": {
        "id": "b4a7e915"
      },
      "outputs": [],
      "source": [
        "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "distribution = df[categories].sum().sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc97aa7",
      "metadata": {
        "id": "bbc97aa7"
      },
      "outputs": [],
      "source": [
        "# Initialize the plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot the distribution of comments per category\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x=distribution.values, y=distribution.index)\n",
        "plt.title('Distribution of Comments by Category')\n",
        "plt.xlabel('Number of Comments')\n",
        "plt.ylabel('Category')\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3291b0",
      "metadata": {
        "id": "7d3291b0"
      },
      "source": [
        "The distribution of comments by category shows that most comments are classified as non-harmful in all categories. However, the toxic category has the highest number of occurrences, followed by obscene and insult. The severe_toxic, threat and identity_hate categories have significantly fewer comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb30714",
      "metadata": {
        "id": "cfb30714"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of each comment\n",
        "df['comment_length'] = df['cleaned_comment_text'].apply(len)\n",
        "# Calculate the total number of damaging vs non-damaging comments\n",
        "df['is_harmful'] = df[categories].sum(axis=1) > 0\n",
        "harmful_distribution = df['is_harmful'].value_counts()\n",
        "\n",
        "# Calculate the length of harmful vs non-harmful comments\n",
        "harmful_comment_length = df[df['is_harmful']]['comment_length']\n",
        "non_harmful_comment_length = df[~df['is_harmful']]['comment_length']\n",
        "\n",
        "# Initialize the plot\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Plot the distribution of harmful vs non-harmful comments\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.barplot(x=harmful_distribution.index, y=harmful_distribution.values, palette='viridis')\n",
        "plt.title('Distribution of Harmful vs Non-Harmful Comments')\n",
        "plt.xlabel('harmful')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.xticks([0, 1], ['Non-harmful', 'harmful'])\n",
        "\n",
        "# Plot the distribution of lengths for harmful comments\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(harmful_comment_length, bins=50, color='red', label='Damaging')\n",
        "plt.title('Length of harmful Comments')\n",
        "plt.xlabel('Comment Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 2000)\n",
        "\n",
        "# Plot the distribution of lengths for non-harmful comments\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(non_harmful_comment_length, bins=50, color='green', label='Non-harmful')\n",
        "plt.title('Length of Non-harmful Comments')\n",
        "plt.xlabel('Comment Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 2000)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c6da81",
      "metadata": {
        "id": "73c6da81"
      },
      "source": [
        "The distribution of comments in the dataset highlights a clear imbalance, with non-harmful comments significantly outnumbering harmful ones. Approximately 140,000 comments are classified as non-harmful, while only around 20,000 are labeled as harmful. This disparity suggests that the majority of the dataset consists of non-harmful content, with harmful comments representing a relatively small portion of the data.\n",
        "\n",
        "Analyzing the lengths of harmful comments reveals that they are predominantly short, with most falling within the 0-250 character range. The frequency of harmful comments decreases sharply as their length increases, and very few exceed 750 characters. The distribution is notably right-skewed, emphasizing the brevity of harmful content.\n",
        "\n",
        "Similarly, non-harmful comments also tend to be short, with the majority concentrated in the 0-250 character range. However, the decline in frequency as comment length increases is more gradual compared to harmful comments. Non-harmful comments have a longer tail in their distribution, indicating a greater presence of longer comments. While the distribution is also right-skewed, it shows more variation in length, with more substantial frequencies in the middle ranges.\n",
        "\n",
        "In summary, the dataset predominantly consists of shorter comments for both harmful and non-harmful categories. However, non-harmful comments exhibit a greater range of lengths, suggesting more variability in their content. Despite the imbalance in the overall distribution, the data effectively captures the general characteristics of both types of comments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "boD4Z4MijWqo",
      "metadata": {
        "id": "boD4Z4MijWqo"
      },
      "source": [
        "## Text Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dwhEMClSjRnd",
      "metadata": {
        "id": "dwhEMClSjRnd"
      },
      "source": [
        "To address class imbalance in the toxic comment classification dataset, i implemented several text augmentation techniques. The augmentation process was carefully designed to maintain the semantic meaning of the original texts while creating diverse variations of the minority class samples.\n",
        "\n",
        "1. Synonym Replacement\n",
        "\n",
        "Randomly selects words from the text and replaces them with their synonyms\n",
        "Uses WordNet from NLTK to find appropriate synonyms\n",
        "Preserves the original meaning while introducing lexical variation\n",
        "\n",
        "2. Random Insertion\n",
        "\n",
        "Identifies synonyms of random words in the text\n",
        "Inserts these synonyms at random positions\n",
        "Increases text length while maintaining semantic coherence\n",
        "\n",
        "3. Random Swap\n",
        "\n",
        "Randomly selects pairs of words in the text\n",
        "Swaps their positions\n",
        "Creates syntactic variations while preserving all original words\n",
        "\n",
        "4. Random Deletion\n",
        "\n",
        "Randomly removes words with a specified probability\n",
        "Helps create shorter variations of the text\n",
        "Maintains core meaning while varying text length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WvNe2hRqfmrj",
      "metadata": {
        "id": "WvNe2hRqfmrj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"Get synonyms of a word using WordNet.\"\"\"\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.name() != word and '_' not in lemma.name():\n",
        "                synonyms.append(lemma.name())\n",
        "    return list(set(synonyms))\n",
        "\n",
        "def synonym_replacement(text, n=1):\n",
        "    \"\"\"Replace n random words with their synonyms.\"\"\"\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if len(get_synonyms(word)) > 0]))\n",
        "\n",
        "    n = min(n, len(random_word_list))\n",
        "    if n == 0:\n",
        "        return text\n",
        "\n",
        "    for _ in range(n):\n",
        "        random_word = random.choice(random_word_list)\n",
        "        random_synonym = random.choice(get_synonyms(random_word))\n",
        "        random_idx = random.randrange(len(new_words))\n",
        "        new_words[random_idx] = random_synonym\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_insertion(text, n=1):\n",
        "    \"\"\"Insert n random synonyms of random words in the text.\"\"\"\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        add_word = random.choice(words)\n",
        "        synonyms = get_synonyms(add_word)\n",
        "        if synonyms:\n",
        "            random_synonym = random.choice(synonyms)\n",
        "            random_idx = random.randrange(len(new_words))\n",
        "            new_words.insert(random_idx, random_synonym)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_swap(text, n=1):\n",
        "    \"\"\"Randomly swap the positions of n pairs of words.\"\"\"\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        if len(new_words) >= 2:\n",
        "            idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
        "            new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_deletion(text, p=0.1):\n",
        "    \"\"\"Randomly delete words from the text with probability p.\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        return text\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.random() > p:\n",
        "            new_words.append(word)\n",
        "    if len(new_words) == 0:\n",
        "        return random.choice(words)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_text(text, num_augmentations=4):\n",
        "    \"\"\"Apply multiple augmentation techniques to generate new samples.\"\"\"\n",
        "    augmentation_functions = [\n",
        "        synonym_replacement,\n",
        "        random_insertion,\n",
        "        random_swap,\n",
        "        lambda x: random_deletion(x, p=0.1)\n",
        "    ]\n",
        "\n",
        "    augmented_texts = []\n",
        "    for _ in range(num_augmentations):\n",
        "        # Randomly choose an augmentation function\n",
        "        aug_func = random.choice(augmentation_functions)\n",
        "        augmented_texts.append(aug_func(text))\n",
        "\n",
        "    return augmented_texts\n",
        "\n",
        "def balance_dataset(df, categories, num_augmentations=2):\n",
        "    \"\"\"Balance the dataset by augmenting minority classes.\"\"\"\n",
        "    # Initialize dictionary to store balanced data\n",
        "    balanced_data = {\n",
        "        'text': [],\n",
        "        **{cat: [] for cat in categories}\n",
        "    }\n",
        "\n",
        "    # Find the maximum number of samples in any category\n",
        "    max_samples = max(df[categories].sum())\n",
        "\n",
        "    # Process each category\n",
        "    for category in categories:\n",
        "        # Get positive samples for current category\n",
        "        positive_mask = df[category] == 1\n",
        "        positive_samples = df[positive_mask]['cleaned_comment_text'].values\n",
        "        current_samples = len(positive_samples)\n",
        "\n",
        "        # If we need more samples for this category\n",
        "        if current_samples < max_samples:\n",
        "            samples_needed = max_samples - current_samples\n",
        "            augmented_samples = []\n",
        "\n",
        "            # Generate augmented samples\n",
        "            while len(augmented_samples) < samples_needed:\n",
        "                text = random.choice(positive_samples)\n",
        "                new_texts = augment_text(text, num_augmentations=1)  # Generate one augmentation at a time\n",
        "                augmented_samples.extend(new_texts[:min(1, samples_needed - len(augmented_samples))])\n",
        "\n",
        "            # Add original samples\n",
        "            balanced_data['text'].extend(positive_samples)\n",
        "            for cat in categories:\n",
        "                balanced_data[cat].extend([1 if cat == category else 0] * len(positive_samples))\n",
        "\n",
        "            # Add augmented samples\n",
        "            balanced_data['text'].extend(augmented_samples)\n",
        "            for cat in categories:\n",
        "                balanced_data[cat].extend([1 if cat == category else 0] * len(augmented_samples))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    balanced_df = pd.DataFrame(balanced_data)\n",
        "    return balanced_df\n",
        "\n",
        "def prepare_augmented_data(df, categories):\n",
        "    \"\"\"Prepare the data with augmentation only on training set.\"\"\"\n",
        "    # First split for test set\n",
        "    X = df['cleaned_comment_text']\n",
        "    y = df[categories]\n",
        "\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.11, random_state=42)\n",
        "\n",
        "    # Second split for validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.10, random_state=42)\n",
        "\n",
        "    # Create temporary DataFrame for training set\n",
        "    train_df = pd.DataFrame({\n",
        "        'cleaned_comment_text': X_train.reset_index(drop=True),\n",
        "        **{cat: y_train[cat].reset_index(drop=True) for cat in categories}\n",
        "    })\n",
        "\n",
        "    # Apply data augmentation only to training set\n",
        "    balanced_train_df = balance_dataset(train_df, categories)\n",
        "\n",
        "    # Merge original training data with augmented data\n",
        "    augmented_train_df = pd.concat([train_df, balanced_train_df], ignore_index=True)\n",
        "\n",
        "    # Prepare final data\n",
        "    X_train_aug = augmented_train_df['text'] if 'text' in augmented_train_df.columns else augmented_train_df['cleaned_comment_text']\n",
        "    y_train_aug = augmented_train_df[categories]\n",
        "\n",
        "    return (X_train_aug, X_val, X_test), (y_train_aug, y_val, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q5xHGhfsf1rZ",
      "metadata": {
        "id": "q5xHGhfsf1rZ"
      },
      "outputs": [],
      "source": [
        "# Definire le categorie\n",
        "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Ottenere i set di dati con augmentation solo sul training set\n",
        "(X_train, X_val, X_test), (Y_train, Y_val, Y_test) = prepare_augmented_data(df, categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788d1b80",
      "metadata": {
        "id": "788d1b80"
      },
      "outputs": [],
      "source": [
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5612d26",
      "metadata": {
        "id": "b5612d26"
      },
      "source": [
        "Inverse class weights are calculated based on the class frequencies in the training set. This approach addresses class imbalance by assigning higher weights to underrepresented classes, ensuring they have a greater influence during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca49d562",
      "metadata": {
        "id": "ca49d562"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(y_train):\n",
        "    \"\"\"\n",
        "    Calculates inverse class weights based on the frequencies of classes in the training set. This is often used to\n",
        "    handle imbalanced datasets by giving more weight to underrepresented classes during model training.\n",
        "\n",
        "    :param y_train: A pandas DataFrame or numpy array containing the training labels.\n",
        "                    Each column represents a class, and each row represents a sample with binary indicators for class membership.\n",
        "    :return: A dictionary with class indices as keys and calculated inverse weights as values.\n",
        "    \"\"\"\n",
        "    # Calculate the frequency of each class\n",
        "    class_frequencies = y_train.sum(axis=0)\n",
        "\n",
        "    # Calculate inverse weights for each class\n",
        "    weights = [len(y_train) / (len(class_frequencies) * frequency) if frequency > 0 else 0 for frequency in class_frequencies]\n",
        "\n",
        "    # Create a dictionary mapping class index to its weight\n",
        "    class_weights = {i: weight for i, weight in enumerate(weights)}\n",
        "\n",
        "    # Optionally, you can print the calculated weights\n",
        "    #print(\"Calculated weights for classes:\", class_weights)\n",
        "\n",
        "    return class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f409f0d1",
      "metadata": {
        "id": "f409f0d1"
      },
      "outputs": [],
      "source": [
        "class_weights = calculate_class_weights(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8424f15",
      "metadata": {
        "id": "d8424f15"
      },
      "outputs": [],
      "source": [
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab47c32",
      "metadata": {
        "id": "aab47c32"
      },
      "source": [
        "### Text Vecorization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e3cbd1",
      "metadata": {
        "id": "01e3cbd1"
      },
      "source": [
        "***Important preprocessing step when working with text data in machine learning because it allows the text data to be transformed into numerical data that can be used as input to machine learning models.***\n",
        "\n",
        "- One common technique for vectorizing text is through tokenization, which involves breaking the text into smaller units called tokens, such as words or phrases. Once tokenized, these units can then be mapped to numerical values to create vector representations of the text.\n",
        "\n",
        "- In this context, the Keras `Tokenizer` class provides a useful tool for tokenizing and vectorizing text data. By using the `fit_on_texts()` method of the Tokenizer object, the internal vocabulary of the tokenizer can be updated based on the frequency of each word in the text data.\n",
        "\n",
        "- It is important to **fit the tokenizer only on the training set** because we want to learn the vocabulary and the word index based on the training data. If we fit the tokenizer also on the test set, we would be introducing bias into our model. The model would have knowledge of the test set and could potentially overfit to it, resulting in poorer performance on unseen data.\n",
        "\n",
        "- After fitting the tokenizer on the training data, we can use `texts_to_sequences()` method to convert the texts, in both the training, validation and test data, to sequences of integers representing the token indices of the words in the original text. This ensures that the same vocabulary is used for both the training and test data, which is important for the model to generalize well to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JCCnDU_hhphZ",
      "metadata": {
        "id": "JCCnDU_hhphZ"
      },
      "outputs": [],
      "source": [
        "# Convert all values to strings and handle NaN values\n",
        "def clean_text_data(text_series):\n",
        "    # Convert to string and replace NaN with empty string\n",
        "    return text_series.fillna('').astype(str)\n",
        "\n",
        "# Clean all text datasets\n",
        "X_train = clean_text_data(X_train)\n",
        "X_val = clean_text_data(X_val)\n",
        "X_test = clean_text_data(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ce45a3",
      "metadata": {
        "id": "c6ce45a3"
      },
      "outputs": [],
      "source": [
        "X_tokenizer = Tokenizer()\n",
        "X_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "#sequences\n",
        "X_sequences_train = X_tokenizer.texts_to_sequences(X_train)\n",
        "X_sequences_val = X_tokenizer.texts_to_sequences(X_val)\n",
        "X_sequences_test = X_tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd154846",
      "metadata": {
        "id": "dd154846"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805d281a",
      "metadata": {
        "id": "805d281a"
      },
      "source": [
        "**Padding involves adding zeros or other placeholder values to the end of shorter sequences to make all sequences in a dataset of equal length, either by padding them to the length of the longest sequence in the dataset or to a fixed length specified by hand. This is important because deep learning model such as Recurrent Neural Networks (RNNs) or Transformers (e.g. BERT) require inputs of a fixed size. Padding ensures that all sequences have the same length, allowing these models to process them in a consistent manner. ***\n",
        "\n",
        "- typically need to use the `pad_sequences()` function from Keras to pad the sequences to a fixed length.\n",
        "\n",
        "- However, the fixed length can be considerd as a hyperparamter and we need to choose carefully this value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d30fcf",
      "metadata": {
        "id": "31d30fcf"
      },
      "outputs": [],
      "source": [
        "maxlen = len(max(X_sequences_train,key=len))\n",
        "\n",
        "#train padding\n",
        "padded_X_sequences_train = pad_sequences(X_sequences_train, padding = 'pre', maxlen = maxlen)\n",
        "\n",
        "#val padding\n",
        "padded_X_sequences_val = pad_sequences(X_sequences_val, padding = 'pre', maxlen = maxlen)\n",
        "\n",
        "# test padding\n",
        "padded_X_sequences_test = pad_sequences(X_sequences_test, padding='pre', maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e729c05",
      "metadata": {
        "id": "1e729c05"
      },
      "outputs": [],
      "source": [
        "# Dimesnion Check\n",
        "print(f\"Train sequences shape: {padded_X_sequences_train.shape}\")\n",
        "print(f\"Validation sequences shape: {padded_X_sequences_val.shape}\")\n",
        "print(f\"Test sequences shape: {padded_X_sequences_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a7ddcb",
      "metadata": {
        "id": "b3a7ddcb"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(X_tokenizer.word_index)+1\n",
        "print(\"Max sentence length: {}\".format(padded_X_sequences_train.shape[1]))\n",
        "print(\"vocabulary size: {}\".format(vocab_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c868db5c",
      "metadata": {
        "id": "c868db5c"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03078e40",
      "metadata": {
        "id": "03078e40"
      },
      "source": [
        "I use as model the multichannel convolutional bidirectional gated recurrent unit for multilabel toxic category detection proposed by Kumar et al. (2021) [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96192eca",
      "metadata": {
        "id": "96192eca"
      },
      "source": [
        "This model extends the standard CNN by using multiple channels with different kernel sizes, enabling the simultaneous processing of various n-grams (e.g., 1-gram, 2-gram, 3-grams). The architecture includes a word embedding layer, a 1D-convolutional layer, dropout, max-pooling, a bidirectional GRU layer, and another dropout layer. It is designed to capture diverse linguistic patterns effectively across different channels.\n",
        "\n",
        "1. Multichannel Word Embedding\n",
        "Words are preprocessed by removing punctuation and special symbols, then converted into numeric vectors using pre-trained GloVe embeddings (100 dimensions). In the multichannel setup, word embeddings are generated with varying contexts or window sizes for each channel. This enables parallel extraction of features from the same training data. Notably, the embeddings are not updated during training, ensuring consistency in semantic representation.\n",
        "\n",
        "2. Convolutional Neural Network (CNN)\n",
        "The CNN layer applies multiple filters over the input sequence to generate feature maps. Each channel uses a 1D-CNN with a set of filters and varying kernel sizes, producing new feature maps by scanning the input sequence. These maps capture local patterns, forming the basis for downstream processing.\n",
        "\n",
        "3. Pooling Layers\n",
        "Pooling layers reduce the dimensions of feature maps to focus on the most important features. Maximum pooling is applied across channels, selecting the maximum value from local neighborhoods of each feature map. This step ensures dimensionality reduction while retaining key information.\n",
        "\n",
        "4. Bidirectional Gated Recurrent Unit (GRU)\n",
        "The bidirectional GRU layer processes input sequences in both forward and backward directions, capturing semantic information from both past and future contexts. GRUs are computationally more efficient than LSTMs and rely on update and reset gates to manage memory. When combined with CNN and pooling layers, this architecture effectively models sequential dependencies while reducing computational complexity.\n",
        "\n",
        "5. Output Layer\n",
        "The outputs from all channels are concatenated and passed through dense and normalization layers. Dense layers adjust vector dimensions to optimize trainable parameters, while normalization layers ensure efficient learning at each network layer. Finally, a sigmoid activation function in the output layer predicts multilabel categories using binary cross-entropy loss to determine whether a comment belongs to one or more categories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59f1528",
      "metadata": {
        "id": "a59f1528"
      },
      "source": [
        "![FlowChart.jpg](attachment:FlowChart.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c919397",
      "metadata": {
        "id": "6c919397"
      },
      "source": [
        "![Hyperparameters.jpg](attachment:Hyperparameters.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6007f431",
      "metadata": {
        "id": "6007f431"
      },
      "source": [
        "### Multichannel Word Embedding Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b445c6",
      "metadata": {
        "id": "67b445c6"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('GloVe data loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cc8f8e",
      "metadata": {
        "id": "75cc8f8e"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, index in X_tokenizer.word_index.items():\n",
        "    if index < vocab_size:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "        else:\n",
        "\n",
        "            embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim,))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9db9d0",
      "metadata": {
        "id": "3a9db9d0"
      },
      "source": [
        "### Creation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a3589e",
      "metadata": {
        "id": "47a3589e"
      },
      "outputs": [],
      "source": [
        "clear_session()\n",
        "embedding_dim = 100\n",
        "filters = 128\n",
        "kernel_sizes = [1, 2, 3, 5, 6]\n",
        "gru_units = 200\n",
        "dropout_rate = 0.6\n",
        "pool_size = 4\n",
        "output_units = 6\n",
        "\n",
        "# Input Layer\n",
        "input_layer = Input(shape=(maxlen,))\n",
        "\n",
        "branches = []\n",
        "for kernel_size in kernel_sizes:\n",
        "    # Embedding layer\n",
        "    embedding = Embedding(input_dim=vocab_size,\n",
        "                          output_dim=embedding_dim,\n",
        "                          weights=[embedding_matrix],\n",
        "                          input_length=maxlen,\n",
        "                          trainable=False)(input_layer)\n",
        "\n",
        "    # 1D CNN\n",
        "    cnn = Conv1D(filters=filters,\n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu')(embedding)\n",
        "\n",
        "    # Dropout\n",
        "    cnn = Dropout(dropout_rate)(cnn)\n",
        "\n",
        "    # Max Pooling\n",
        "    pooled = MaxPooling1D(pool_size=pool_size)(cnn)\n",
        "\n",
        "    # BiGRU\n",
        "    bigru = Bidirectional(GRU(gru_units, return_sequences=False))(pooled)\n",
        "\n",
        "    # Dropout\n",
        "    bigru = Dropout(dropout_rate)(bigru)\n",
        "\n",
        "    branches.append(bigru)\n",
        "\n",
        "# Concatentation\n",
        "concatenated = concatenate(branches)\n",
        "\n",
        "# Fully Connected Layer\n",
        "dense = Dense(128, activation='relu')(concatenated)\n",
        "\n",
        "# Batch Normalization\n",
        "normalized = BatchNormalization()(dense)\n",
        "\n",
        "# Output Layer\n",
        "output_layer = Dense(output_units, activation='sigmoid')(normalized)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compilation\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = [tfa.metrics.F1Score(num_classes=6, average='macro', threshold=0.5)])\n",
        "              #metrics=['accuracy'])\n",
        "\n",
        "# Summary model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e23672",
      "metadata": {
        "id": "44e23672"
      },
      "source": [
        "For the hyperparameters, reference was made to the table extracted from the article. Only two modifications were made: the number of epochs was set to 2 (unlike the article, which specifies 100), and the batch size was set to 216. This choice was driven by the computational resources available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0531ced6",
      "metadata": {
        "id": "0531ced6"
      },
      "outputs": [],
      "source": [
        "history = model.fit(padded_X_sequences_train, Y_train,\n",
        "                    validation_data=(padded_X_sequences_val, Y_val),\n",
        "                    epochs=2,\n",
        "                    batch_size=216,\n",
        "                    class_weight= class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71efbb55",
      "metadata": {
        "id": "71efbb55"
      },
      "outputs": [],
      "source": [
        "# To save the model\n",
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e5f6f8",
      "metadata": {
        "id": "a7e5f6f8"
      },
      "outputs": [],
      "source": [
        "#To load the model\n",
        "model = tf.keras.models.load_model('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008dee14",
      "metadata": {
        "id": "008dee14"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf81c7be",
      "metadata": {
        "id": "cf81c7be"
      },
      "outputs": [],
      "source": [
        "# Plot the loss and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the F1 macro score on the training and validation sets\n",
        "plt.plot(history.history['f1_score'], label='Training F1')\n",
        "plt.plot(history.history['val_f1_score'], label='Validation F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 score Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3d0rPpkTsndL"
      },
      "id": "3d0rPpkTsndL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4e2276",
      "metadata": {
        "id": "7a4e2276"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "y_pred = model.predict(padded_X_sequences_test)\n",
        "\n",
        "test_loss, test_f1_score = model.evaluate(padded_X_sequences_test, Y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test f1 score: {test_f1_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb09919",
      "metadata": {
        "id": "6cb09919"
      },
      "outputs": [],
      "source": [
        "# Binarize the predictions using a threshold (default is 0.5)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Ensure the data is in NumPy array format\n",
        "Y_test = np.array(Y_test)\n",
        "y_pred_binary = np.array(y_pred_binary)\n",
        "\n",
        "# Set up the layout for the figure (3 columns x 2 rows)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns\n",
        "axes = axes.flatten()  # Flatten the array for easier indexing\n",
        "\n",
        "# Compute and visualize the confusion matrix for each label\n",
        "for i in range(min(6, Y_test.shape[1])):  # Assume a maximum of 6 labels\n",
        "    print(f\"Confusion matrix for label {i+1}:\")\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(Y_test[:, i], y_pred_binary[:, i])\n",
        "\n",
        "    # Display the confusion matrix in the corresponding subplot\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0\", \"1\"])\n",
        "    disp.plot(ax=axes[i], cmap=plt.cm.Blues)\n",
        "    axes[i].set_title(f\"Label {i+1}\")\n",
        "\n",
        "# Add a main title to the figure\n",
        "plt.suptitle(\"Confusion Matrices for Each Label\", fontsize=16)\n",
        "\n",
        "# Optimize the layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Create space for the main title\n",
        "\n",
        "# Save the image\n",
        "plt.savefig(\"confusion_matrices_all_labels.png\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b7af81",
      "metadata": {
        "id": "b5b7af81"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645503d3",
      "metadata": {
        "id": "645503d3"
      },
      "source": [
        "1) Kumar, A., Abirami, S., Trueman, T. E., & Cambria, E. (2021). Comment toxicity detection via a multichannel convolutional bidirectional gated recurrent unit. Neurocomputing, 441, 272-278."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7495fe2a",
      "metadata": {
        "id": "7495fe2a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}